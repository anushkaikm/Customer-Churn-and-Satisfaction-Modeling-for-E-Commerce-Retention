# -*- coding: utf-8 -*-
"""DS862_Anushka_Rongxian_ChurnSatisfaction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OW3LawgDVxIkErcyJ-RvUs9A1b658v97

# Problem Statement

This section sets the context for the project. Our goal is to reduce customer churn and increase retention in a competitive e-commerce environment. Predicting churn and modeling customer satisfaction will enable data-driven strategies for customer relationship management.

# Imports & Configuration
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm

from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

# Plot settings
sns.set(style="whitegrid")
plt.rcParams["figure.figsize"] = (10, 6)

"""# Data Reading & Cleaning"""

# We begin by importing the dataset and performing basic cleaning. We drop missing values and standardize inconsistent text labels. Dropping missing rows is acceptable here as the missing data is sparse (~5%) and randomly distributed.

df = pd.read_csv("ecommercedata.csv")
print("First 5 rows of the dataset:")
print(df.head())

print(f"\nDataset shape: {df.shape}")
print("\nMissing values in each column:")
print(df.isnull().sum())
print("\nData types of columns:")
print(df.dtypes)

# Drop duplicates and missing values
df.drop_duplicates(inplace=True)
df_cleaned = df.dropna()
print("\nShape after dropping missing values:", df_cleaned.shape)
df_cleaned.info()

# Standardize 'PreferredPaymentMode'
df_cleaned['PreferredPaymentMode'] = df_cleaned['PreferredPaymentMode'].replace({
    'COD': 'Cash on Delivery',
    'Cash on delivery': 'Cash on Delivery',
    'CC': 'Credit Card'
})
print(df_cleaned['PreferredPaymentMode'].value_counts())

# Clean categorical inconsistencies
df_cleaned['PreferredLoginDevice'] = df_cleaned['PreferredLoginDevice'].replace({
    'Phone': 'Mobile Phone'
}).str.title()
df_cleaned['PreferedOrderCat'] = df_cleaned['PreferedOrderCat'].replace({
    'Mobile': 'Mobile Phone'
})

# Feature Engineering
df_cleaned['AvgOrderValue'] = df_cleaned['CashbackAmount'] / (df_cleaned['OrderCount'] + 1e-5)
df_cleaned['CouponRedemptionRate'] = df_cleaned['CouponUsed'] / (df_cleaned['OrderCount'] + 1e-5)

# Strip any column whitespace
df_cleaned.columns = df_cleaned.columns.str.strip()

"""# Exploratory Data Analysis (EDA)"""

# Univariate Analysis
numeric_cols = ['Tenure', 'CashbackAmount', 'HourSpendOnApp', 'OrderCount', 'OrderAmountHikeFromlastYear']
for col in numeric_cols:
    plt.figure()
    sns.histplot(df_cleaned[col], kde=True, bins=30, color='skyblue')
    plt.title(f'Distribution of {col}')
    plt.show()

    plt.figure()
    sns.boxplot(x=df_cleaned[col], color='lightgreen')
    plt.title(f'Boxplot of {col}')
    plt.show()

categorical_cols = ['PreferredLoginDevice', 'PreferredPaymentMode', 'Gender', 'MaritalStatus', 'PreferedOrderCat', 'CityTier', 'Complain']
for col in categorical_cols:
    plt.figure()
    sns.countplot(data=df_cleaned, x=col, order=df_cleaned[col].value_counts().index, palette='Set2')
    plt.title(f'Count of {col}')
    plt.xticks(rotation=45)
    plt.show()

"""### Interpretation: Distribution of Tenure

The histogram and KDE plot above illustrate the distribution of customer tenure on the platform. We observe a right-skewed distribution, where a large proportion of customers have very short tenures, particularly less than 5 units (likely months). The frequency drops off as tenure increases, indicating that many customers are either new or leave the platform relatively early.

This insight suggests a potential churn issue early in the customer lifecycle. Therefore, retention strategies should prioritize onboarding, first impressions, and early engagement to prevent these short-tenure drop-offs. Understanding the behaviors of long-tenure customers can also inform what works well for retention.

### Interpretation: Boxplot of Tenure

The boxplot provides a summary of the spread and potential outliers in customer tenure. The interquartile range (IQR) indicates that most customers have a tenure between approximately 2 and 20 months. The median tenure appears to be around 9 months.

Notably, we see a few outliers beyond the upper whisker—these are customers with unusually long tenure (around 50 months). These outliers might represent highly loyal customers and could offer valuable insights into long-term engagement strategies.

Overall, the boxplot confirms the skewed distribution observed earlier and suggests that while a few customers are retained for extended periods, the majority churn within the first 1–2 years. This insight supports the need for focused retention efforts in early tenure.

### Interpretation: Distribution of Cashback Amount

The distribution of `CashbackAmount` appears approximately normal but slightly right-skewed, with a strong concentration of values between 130 and 180. The peak is centered around 150, suggesting that most customers receive a similar cashback amount.

There are a few extreme values in the tails, indicating rare cases of customers receiving either very low or unusually high cashback amounts. These may result from special promotions, refunds, or errors.

This relatively tight distribution implies that cashback policies are fairly consistent across customers. The analysis of this variable is important because higher cashback might contribute to satisfaction and retention—but too much could hurt profitability.

### Interpretation: Boxplot of Cashback Amount

The boxplot reveals that most cashback values are concentrated between approximately 130 and 190, with a median around 155. The interquartile range (IQR) is relatively narrow, confirming the consistency seen in the histogram.

However, the presence of several outliers on both ends—particularly very low and very high cashback amounts—suggests that some customers receive amounts outside the typical policy range. These may warrant further inspection to check for special promotions, refunds, or anomalies.

This visualization reinforces the importance of setting appropriate cashback levels to balance customer satisfaction with financial sustainability.

### Interpretation: Distribution of HourSpendOnApp

The distribution of `HourSpendOnApp` shows a distinct multi-modal pattern, with three clear peaks centered around 2, 3, and 4 hours. This suggests that many users spend a fixed or habitual amount of time on the app, possibly reflecting different behavioral clusters or user personas (e.g., casual, moderate, and heavy users).

The dominant peak at 3 hours indicates a large segment of users consistently spend that much time daily, which may represent the typical or target user experience.

This non-normal distribution highlights the need to consider user segments in modeling and marketing strategies, as average-based assumptions may obscure underlying behavioral groups.

### Interpretation: Boxplot of HourSpendOnApp

The boxplot for `HourSpendOnApp` shows a fairly symmetric distribution with a median around 2.5 to 3 hours. The interquartile range spans from approximately 2 to 4 hours, indicating that 50% of users spend between those amounts of time on the app.

There are a few mild outliers on both extremes—some users spend close to 0 or 5 hours, which might represent either minimal engagement or power users.

This visualization supports the earlier histogram by confirming that most users fall within a well-defined range of usage, reinforcing the presence of habitual or structured app interaction behaviors among customers.

### Interpretation: Distribution of OrderCount

The distribution of `OrderCount` is heavily right-skewed, indicating that most users place only a small number of orders. The mode is at 2 orders, and the frequency drops sharply as the order count increases.

This suggests that the majority of customers are low-frequency buyers, with a few making larger numbers of purchases. These infrequent buyers could be targeted with re-engagement strategies such as loyalty rewards, limited-time discounts, or personalized recommendations.

Understanding this distribution helps in segmenting customers into occasional versus repeat buyers, which can guide targeted retention and upsell efforts.

### Interpretation: Boxplot of OrderCount

The boxplot confirms that the majority of users place a small number of orders, with the median at around 3. The interquartile range spans from about 1 to 5 orders, meaning that 50% of customers fall within this range.

There are numerous outliers beyond 6 orders, with values reaching up to 16. These high-order customers could represent loyal, high-value users and should be studied for common traits to replicate through marketing strategies.

The presence of many outliers also suggests potential value in segmenting customers by purchasing frequency to optimize engagement and upselling efforts.

### Interpretation: Distribution of OrderAmountHikeFromlastYear

The distribution of `OrderAmountHikeFromlastYear` shows a slightly right-skewed pattern with a peak around 14. This suggests that most customers experienced a modest increase in their order value from the previous year, clustered between 12 and 16.

There is a gradual decline in frequency as the hike increases, indicating that fewer customers saw significant growth in their order amounts. This tail may represent users who increased their spending substantially, possibly due to new product interests, promotions, or increased loyalty.

This feature is important in modeling, as it reflects customer momentum and could be an early signal of retention or growing value.

### Interpretation: Boxplot of OrderAmountHikeFromlastYear

The boxplot shows that most customers have an order amount hike between roughly 13 and 20, with a median close to 15. This supports the earlier histogram by confirming that the majority of customers experienced moderate growth in order value over the past year.

There is one mild outlier on the upper end (above 25), suggesting a customer or small group whose spending increased significantly—these may be high-potential customers worth further profiling.

Overall, this feature is fairly symmetrical and likely reliable for modeling changes in customer value.

### Interpretation: Count of PreferredLoginDevice

The bar chart shows that the majority of customers prefer to log in using a **mobile phone** rather than a **computer**. This mobile-first behavior aligns with modern e-commerce trends, where convenience and accessibility drive engagement.

This insight is actionable—it suggests that optimizing the mobile app experience (e.g., speed, UI/UX, in-app promotions) could significantly enhance customer satisfaction and retention.

Furthermore, the smaller segment of computer users might represent a different customer demographic, potentially with different purchasing habits or expectations.

### Interpretation: Count of PreferredPaymentMode

This bar chart shows that **Debit Card** is the most preferred payment method among customers, followed by **Credit Card**. Digital wallets (E-wallet), **Cash on Delivery**, and **UPI** are used by smaller customer segments.

The dominance of card-based transactions suggests that customers value convenience, speed, and security. The lower usage of UPI and cash-based options may indicate opportunities for incentivizing these channels (e.g., offering discounts for UPI to reduce payment gateway fees or improve delivery experience).

This insight can help the business tailor promotions or payment partnerships based on customer behavior.

### Interpretation: Count of Gender

The chart shows a higher number of **male** customers compared to **female** customers in the dataset. While both genders are well represented, this imbalance may reflect differences in platform appeal, marketing reach, or product preferences.

Understanding this distribution is important for designing gender-sensitive strategies. For example, if customer satisfaction or churn differs significantly between genders, targeted campaigns or tailored messaging could help close the engagement gap and optimize retention.

### Interpretation: Count of MaritalStatus

The chart indicates that **married** customers represent the largest segment, followed by **single**, and then **divorced** individuals. This demographic breakdown can offer insights into lifestyle and purchasing behavior.

For example, married customers may shop more frequently or for households, while single customers might prioritize convenience or entertainment. Understanding these groups helps tailor product offerings, messaging, and retention strategies accordingly.

This variable can also be useful for segmentation in churn or satisfaction prediction models.

### Interpretation: Count of PreferedOrderCat

The bar chart shows that **Laptop & Accessory** is the most frequently preferred order category, followed by **Mobile Phone**. Other categories such as **Fashion**, **Others**, and **Grocery** are significantly less popular.

This suggests that the platform may primarily attract tech-savvy customers or be positioned more strongly in the electronics vertical. These insights can inform inventory planning, category-specific promotions, or homepage design.

The low engagement with Fashion and Grocery categories could indicate either limited offerings or lower demand, which could be areas for exploration or expansion.

### Interpretation: Count of CityTier

The chart reveals that most customers come from **Tier 1** cities, followed by **Tier 3**, with **Tier 2** cities having the fewest users. This distribution may reflect differences in population density, internet penetration, income level, or marketing reach.

The dominance of Tier 1 customers indicates strong urban adoption, while the relatively small representation from Tier 2 and 3 areas suggests an opportunity for geographic market expansion or targeted outreach campaigns.

This variable can also be important for understanding behavior differences across urbanization levels, such as spending habits or preferred product categories.






"""

# Bivariate Analysis
corr_cols = ['Tenure', 'CashbackAmount', 'HourSpendOnApp', 'OrderCount', 'OrderAmountHikeFromlastYear', 'CouponUsed', 'DaySinceLastOrder']
plt.figure(figsize=(10, 8))
sns.heatmap(df_cleaned[corr_cols].corr(), annot=True, cmap="coolwarm")
plt.title('Correlation Heatmap')
plt.show()

cat_vars = ['Churn', 'Gender', 'MaritalStatus', 'PreferredLoginDevice']
num_vars = ['CashbackAmount', 'HourSpendOnApp', 'OrderCount']
for cat in cat_vars:
    for num in num_vars:
        plt.figure(figsize=(6, 4))
        sns.boxplot(x=df_cleaned[cat], y=df_cleaned[num])
        plt.title(f'{num} vs {cat}')
        plt.tight_layout()
        plt.show()

"""### Interpretation: Correlation Heatmap

The correlation heatmap shows relationships among the key numerical features in the dataset. Most correlations are modest, indicating that features contribute relatively independently to the modeling process.

Notable observations:
- `OrderCount` and `CouponUsed` show a strong positive correlation (0.73), which is intuitive—more orders typically lead to more opportunities to redeem coupons.
- `OrderCount` is moderately correlated with `DaySinceLastOrder` (0.46), possibly reflecting recent activity from frequent shoppers.
- Most other correlations are below 0.3, meaning multicollinearity is not a major concern, and features like `Tenure`, `CashbackAmount`, and `HourSpendOnApp` provide distinct information.

This heatmap is useful for selecting features, checking redundancy, and interpreting model behavior.

### Interpretation: CashbackAmount vs Churn

The boxplot compares cashback amounts between customers who churned (`Churn = 1`) and those who stayed (`Churn = 0`). Both groups have similar median cashback values and interquartile ranges, with overlapping distributions.

This suggests that **cashback amount alone may not be a strong differentiator** between churned and retained customers. However, both groups exhibit outliers—some users in each group received unusually high cashback, potentially linked to promotional activity or high-value purchases.

This finding implies that while cashback may influence satisfaction, it is likely not the sole driver of churn. It should be analyzed in combination with behavioral or experiential features.

### Interpretation: HourSpendOnApp vs Churn

This boxplot compares time spent on the app between churned and non-churned customers. Surprisingly, customers who **churned** tend to have spent **more time** on the app (median ~4 hours) compared to those who stayed (median ~3 hours).

This pattern may indicate that time spent doesn't always translate to satisfaction—higher usage without reward or satisfaction could lead to frustration and eventual churn. It’s also possible that heavier users become more critical or have higher expectations.

This variable, while counterintuitive, could be a valuable churn predictor when combined with qualitative feedback or satisfaction scores.

### Interpretation: OrderCount vs Churn

This boxplot compares the number of orders placed by churned and retained customers. Interestingly, the median `OrderCount` is slightly higher for churned users than for those who stayed, and both groups have similar IQRs.

The long tail of outliers in both segments indicates that some customers are high-frequency buyers regardless of churn status. However, the slightly elevated central tendency among churners might suggest that **order frequency alone does not guarantee retention**—perhaps expectations rise with usage, and unmet expectations lead to churn.

This variable should be used in combination with satisfaction metrics and recency indicators for more accurate churn modeling.

### Interpretation: CashbackAmount vs Gender

This boxplot compares the distribution of cashback amounts between male and female customers. The distributions are nearly identical, with similar medians, interquartile ranges, and spread of outliers.

This suggests that cashback policies are applied consistently across genders, and there’s no evidence of systematic bias in how cashback is distributed. This parity supports fairness in promotional strategies and avoids introducing gender-based skew into predictive models.

Still, it's worth checking if cashback effectiveness (e.g., impact on churn or satisfaction) differs by gender.

### Interpretation: HourSpendOnApp vs Gender

This boxplot shows that **female** customers tend to spend slightly more time on the app than **male** customers. The median usage for females is higher, and their distribution is slightly wider, suggesting more variability in engagement levels.

This pattern could reflect different browsing or shopping behaviors between genders. It may also suggest that female users are more exploratory or engaged, which could influence how features, recommendations, and promotions are designed.

Such behavioral differences should be considered when designing personalized experiences or analyzing satisfaction and churn across demographics.

### Interpretation: OrderCount vs Gender

This boxplot reveals that the number of orders placed is nearly identical across male and female customers. Both groups have similar medians, interquartile ranges, and a long tail of outliers, indicating that **gender does not significantly influence order frequency**.

This finding supports the idea that purchasing behavior, at least in terms of quantity, is fairly balanced between genders on this platform. It suggests that promotions and retention strategies related to order frequency can likely be designed in a gender-neutral way.

### Interpretation: CashbackAmount vs MaritalStatus

This boxplot compares cashback distributions across marital status groups. The median cashback amount is similar for **Single**, **Divorced**, and **Married** customers, with slight variations in spread and outliers.

The consistency across groups suggests that marital status does not influence how much cashback customers receive, pointing to a uniformly applied cashback policy.

However, the wider spread and more outliers in the **Married** group could hint at a broader range of purchasing behavior—possibly due to household spending—which could be worth exploring further in segmentation analysis.

### Interpretation: HourSpendOnApp vs MaritalStatus

The boxplot reveals differences in app usage across marital status groups. On average, **Married** and **Divorced** customers spend more time on the app than **Single** customers, with slightly higher medians and upper quartiles.

This may suggest that more time-constrained or household-oriented users (e.g., married individuals) rely more heavily on the app for convenience. Alternatively, these groups might be more engaged or responsive to app features and promotions.

These behavioral differences by marital status can inform personalization, notification timing, and segmentation strategies.

### Interpretation: OrderCount vs MaritalStatus

This boxplot shows that **Divorced** and **Married** customers tend to place slightly more orders than **Single** customers, as indicated by higher medians and a longer upper range.

All groups have substantial outliers, indicating the presence of high-frequency buyers across demographics. However, the distributions suggest that life stage and household responsibilities may influence purchase behavior, with married/divorced customers potentially shopping for more than just themselves.

This insight supports segmenting promotional strategies by life stage to drive order volume.

### Interpretation: CashbackAmount vs PreferredLoginDevice

This boxplot compares cashback amounts between users who prefer **mobile phones** versus **computers**. The distributions are nearly identical in terms of median, spread, and outliers, indicating that cashback benefits are platform-neutral.

This suggests that customers receive consistent incentives regardless of how they access the platform. While good from a fairness perspective, it may also mean there’s an opportunity to offer **device-specific promotions** (e.g., app-exclusive cashback) to influence channel behavior.

Knowing that behavior is similar across platforms can also help simplify segmentation and model features.

### Interpretation: HourSpendOnApp vs PreferredLoginDevice

The boxplot shows nearly identical distributions of app usage time between users who prefer **Mobile Phone** and those who prefer **Computer**. Both groups have similar medians and interquartile ranges, indicating that device type does not substantially influence time spent on the app.

This parity suggests a well-optimized user experience across platforms. It also implies that time-based engagement metrics can be treated similarly regardless of device, simplifying feature engineering and reducing the need for device-specific behavioral segmentation.

### Interpretation: OrderCount vs PreferredLoginDevice

This boxplot illustrates that order frequency is nearly identical for customers who use **Mobile Phones** and those who use **Computers** to log in. The medians, spread, and range of outliers are consistent across both device types.

This suggests that purchasing behavior, in terms of number of orders, is not significantly influenced by login device. It reinforces that customers are equally comfortable transacting on either platform, and device preference alone is unlikely to predict order volume.

This finding simplifies modeling efforts, as it reduces the need to treat device type as a behavioral differentiator in this context.


"""

# Categorical-Categorical: Crosstab and Barplot
ct = pd.crosstab(df_cleaned['Churn'], df_cleaned['Complain'], normalize='index')
ct.plot(kind='bar', stacked=True, colormap='Set2')
plt.title("Churn vs Complaint Raised")
plt.ylabel("Proportion")
plt.show()

"""### Interpretation: Churn vs Complaint Raised

This stacked bar chart shows a clear relationship between complaint activity and churn behavior. Among customers who **did not churn**, the majority (about 77%) **did not file a complaint**. In contrast, among those who **did churn**, a significantly larger proportion **had raised a complaint**.

This strongly suggests that complaint behavior is a key indicator of dissatisfaction and a potential early warning sign for churn. Businesses should pay close attention to complaints—not only to resolve issues but also to trigger proactive retention strategies before customers decide to leave.

"""

# Churn by CityTier
churn_by_city = df_cleaned.groupby('CityTier')['Churn'].mean().reset_index()
sns.barplot(x='CityTier', y='Churn', data=churn_by_city)
plt.title("Churn Rate by City Tier")
plt.ylabel("Churn Rate")
plt.show()

"""### Interpretation: Churn Rate by City Tier

This bar chart reveals a clear trend: churn rate **increases** as the city tier **decreases** in affluence or infrastructure. Customers in **Tier 3 cities** exhibit the highest churn rate (over 20%), followed by Tier 2, with **Tier 1 cities showing the lowest churn**.

This pattern may reflect differences in service expectations, internet reliability, product-market fit, or customer experience based on urban infrastructure. It also suggests that retention strategies may need to be more aggressive or customized for lower-tier cities.

Incorporating city tier into churn models and customer segmentation can improve predictive power and help guide geographic targeting for retention efforts.

"""

# Multivariate: Pairplot
pairplot_cols = corr_cols + ['Churn']
sns.pairplot(df_cleaned[pairplot_cols], hue='Churn', palette='Set1')
plt.suptitle("Pairplot of Features by Churn", y=1.02)
plt.show()

"""### Interpretation: Pairplot of Features by Churn

This pairplot shows the relationships between multiple numeric features, color-coded by churn status (red = churned, blue = retained). Each diagonal plot shows the distribution of a single feature, while the scatter plots illustrate pairwise relationships.

Key observations:
- **OrderCount and CouponUsed** are strongly correlated, especially among churned users.
- **DaySinceLastOrder** tends to be higher among churned customers, suggesting recency of engagement is a strong churn indicator.
- **Tenure** is visibly lower for churned users, reinforcing the idea that new customers are more likely to leave.
- Distributions for features like **CashbackAmount** and **HourSpendOnApp** are relatively similar between churned and non-churned users, indicating weaker separation on their own.

This visualization is helpful for spotting which variables may best distinguish churners and where feature interactions could matter in modeling.

# Hypothesis Testing

We test the hypothesis: Does coupon usage increase the likelihood of placing a high-value order?

We use logistic regression because the target (high vs. non-high order) is binary. This analysis gives us statistical evidence to support or refute business assumptions about promotional strategies.
"""

threshold = df_cleaned['OrderAmountHikeFromlastYear'].quantile(0.75)
df_cleaned['HighOrderValue'] = (df_cleaned['OrderAmountHikeFromlastYear'] >= threshold).astype(int)
X = sm.add_constant(df_cleaned[['CouponUsed']])
y = df_cleaned['HighOrderValue']
logit_model = sm.Logit(y, X)
result = logit_model.fit()
print(result.summary())

"""#### Model Summary:
- **Coefficient for `CouponUsed`**: 0.0347
- **p-value**: 0.096
- **Pseudo R²**: 0.0006 (very low)

#### Interpretation:
- The positive coefficient (+0.0347) suggests that for each additional coupon used, the **log-odds** of placing a high-value order **slightly increase**.
- However, the **p-value of 0.096** is above the common significance threshold (0.05), meaning this effect is **not statistically significant** at the 95% confidence level.
- The model’s **pseudo R² is very low**, indicating that coupon usage alone explains very little variance in high-value order likelihood.

#### Conclusion:
We **fail to reject the null hypothesis**. There is no strong statistical evidence to conclude that increased coupon usage significantly increases the chance of placing a high-value order. This suggests that while coupon usage may play a small role, other factors likely have a greater impact on order value.

#### Business Implication:
This result is helpful in guiding promotion strategy. It implies that **increasing coupon distribution may not be an effective lever** for increasing average order value, and other behavioral or contextual factors should be explored instead.

# Churn Prediction (ML Models)

We use multiple models to predict churn, including Logistic Regression (interpretable baseline), Random Forest (handles non-linearities), and SVM (strong margin classifier).

Models are evaluated using F1-score, and threshold tuning is done to balance precision and recall.

This helps us identify which customers are most at risk, so we can intervene before they leave.
"""

df_model = df_cleaned.copy()
df_model = pd.get_dummies(df_model, columns=['PreferredLoginDevice', 'PreferredPaymentMode', 'PreferedOrderCat', 'MaritalStatus'], drop_first=True)
df_model['Gender'] = LabelEncoder().fit_transform(df_model['Gender'])
df_model = df_model.drop(columns=['CustomerID'])
X = df_model.drop(columns=['Churn'])
y = df_model['Churn']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)

models = {
    "Logistic Regression": (
        Pipeline([('scaler', StandardScaler()), ('clf', LogisticRegression(solver='liblinear'))]),
        {'clf__C': [0.01, 0.1, 1, 10], 'clf__penalty': ['l1', 'l2']}
    ),
    "Random Forest": (
        Pipeline([('clf', RandomForestClassifier(random_state=42))]),
        {'clf__n_estimators': [100, 200], 'clf__max_depth': [5, 10, None], 'clf__min_samples_split': [2, 5]}
    ),
    "SVM": (
        Pipeline([('scaler', StandardScaler()), ('clf', SVC(probability=True))]),
        {'clf__C': [0.1, 1, 10], 'clf__kernel': ['linear', 'rbf'], 'clf__gamma': ['scale', 'auto']}
    )
}

for name, (pipeline, params) in models.items():
    print(f"\nTraining {name}...")
    grid = GridSearchCV(pipeline, params, cv=5, scoring='f1', n_jobs=-1)
    grid.fit(X_train, y_train)
    best_model = grid.best_estimator_
    y_proba = best_model.predict_proba(X_test)[:, 1]

    thresholds = np.arange(0.1, 0.9, 0.05)
    scores = [(t, f1_score(y_test, (y_proba >= t).astype(int))) for t in thresholds]
    best_thresh, best_f1 = max(scores, key=lambda x: x[1])

    y_final = (y_proba >= best_thresh).astype(int)
    print(f"Best Params: {grid.best_params_}")
    print(f"Best Threshold: {best_thresh:.2f} | F1: {best_f1:.4f}")
    print(confusion_matrix(y_test, y_final))
    print(classification_report(y_test, y_final))

    plt.figure()
    plt.plot(thresholds, [f1_score(y_test, (y_proba >= t).astype(int)) for t in thresholds], label='F1')
    plt.plot(thresholds, [precision_score(y_test, (y_proba >= t).astype(int)) for t in thresholds], label='Precision')
    plt.plot(thresholds, [recall_score(y_test, (y_proba >= t).astype(int)) for t in thresholds], label='Recall')
    plt.title(f'{name} Threshold Optimization')
    plt.xlabel('Threshold')
    plt.ylabel('Score')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

"""### Churn Prediction Model Performance & Threshold Optimization

To optimize classification performance, threshold tuning was conducted on all three models—Logistic Regression, Random Forest, and SVM—by analyzing F1 score, precision, and recall across different threshold values.

#### 1. **Logistic Regression**
- **Best Threshold**: 0.35  
- **F1 Score (Churn class)**: 0.65  
- **Precision**: 0.61  
- **Recall**: 0.69  
- **Interpretation**: Logistic Regression provided reasonable recall for churners but lower precision. It is a good baseline model that is interpretable but less performant overall.

#### 2. **Random Forest**
- **Best Threshold**: 0.35  
- **F1 Score (Churn class)**: 0.91  
- **Precision**: 0.91  
- **Recall**: 0.91  
- **Interpretation**: Random Forest significantly outperformed other models in all metrics. It is highly effective at detecting churners with minimal false positives or false negatives. This model is well-suited for production use when accuracy is critical.

#### 3. **Support Vector Machine (SVM)**
- **Best Threshold**: 0.50  
- **F1 Score (Churn class)**: 0.82  
- **Precision**: 0.87  
- **Recall**: 0.78  
- **Interpretation**: SVM also showed strong performance, especially in precision. It may be appropriate in scenarios where false positives are costlier than false negatives.

---

### Summary of Model Choice
| Model               | F1 Score | Precision | Recall | Best Threshold |
|--------------------|----------|-----------|--------|----------------|
| Logistic Regression| 0.65     | 0.61      | 0.69   | 0.35           |
| Random Forest      | **0.91** | **0.91**  | **0.91**| 0.35           |
| SVM (RBF)          | 0.82     | 0.87      | 0.78   | 0.50           |

**Recommendation**: Based on the performance metrics and threshold optimization, **Random Forest** is the best model for churn prediction in this dataset due to its balance and high scores across all metrics.

---

**Business Implication**: Accurate identification of churners allows the business to proactively engage at-risk customers through offers, support, or personalized retention strategies, minimizing lost revenue.

# Customer Satisfaction Modeling

We categorize satisfaction scores into 'Low', 'Medium', and 'High' and treat it as a classification problem. Random Forest, XGBoost, and LightGBM are used for this multiclass classification task.

 Understanding satisfaction drivers allows us to enhance user experience and prioritize improvements.

 Feature importances guide which customer experience areas matter most.
"""

satisfaction_cols = ['WarehouseToHome', 'CashbackAmount', 'Tenure', 'HourSpendOnApp', 'OrderCount', 'OrderAmountHikeFromlastYear', 'DaySinceLastOrder']
df_cleaned[satisfaction_cols] = StandardScaler().fit_transform(df_cleaned[satisfaction_cols])
df_cleaned['SatisfactionCategory'] = pd.cut(df_cleaned['SatisfactionScore'], bins=[0, 2, 3, 5], labels=['Low', 'Medium', 'High'])
df_encoded = pd.get_dummies(df_cleaned.drop(columns=['SatisfactionCategory', 'SatisfactionScore', 'Churn']), drop_first=True)

X = df_encoded
y = df_cleaned['SatisfactionCategory']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'max_features': ['sqrt'],
    'bootstrap': [True, False]
}

rf_model = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, n_jobs=-1, scoring='f1_macro')
grid_search.fit(X_train, y_train)
print("Best RF Params:", grid_search.best_params_)
print(classification_report(y_test, grid_search.predict(X_test)))

"""Overall Accuracy: 88%

Macro F1 Score: 0.87 (balanced across all classes)

Weighted F1 Score: 0.88

###Interpretation:

The model performs well across all satisfaction categories, especially in identifying Low and High satisfaction levels with high precision and recall.

Slightly lower recall in the Medium class suggests mild confusion between Medium and neighboring categories—expected in ordinal classification.

The balanced macro average confirms the model does not overly favor one class, despite class imbalance.

###Business Implication:

This model can accurately classify customer satisfaction using behavioral and demographic features.

It can be deployed to proactively identify dissatisfied customers (Low) for intervention or highly satisfied customers (High) for referral/loyalty campaigns.

Insights from feature importance can also inform which aspects of the customer journey need improvement or investment.

The model is well-suited for decision-making and personalization strategies aimed at increasing customer retention and lifetime value.

"""

importances = pd.Series(grid_search.best_estimator_.feature_importances_, index=X.columns)
importances.nlargest(10).plot(kind='barh')
plt.title("Top Features for Satisfaction Prediction")
plt.xlabel("Importance Score")
plt.tight_layout()
plt.show()

"""### Interpretation: Top Features for Customer Satisfaction Prediction

This feature importance plot shows the most influential variables used by the Random Forest model to predict customer satisfaction levels.

#### Top Features:
- **AvgOrderValue**: The strongest predictor. Customers spending more per order tend to report higher satisfaction—possibly due to more value being delivered.
- **WarehouseToHome**: Indicates logistical efficiency. Shorter delivery times may lead to higher satisfaction.
- **Tenure**: Loyal or long-standing customers tend to be more satisfied, reinforcing the importance of retention.
- **CashbackAmount**: Shows that incentives like cashback contribute positively to perceived value.
- **OrderAmountHikeFromlastYear** and **DaySinceLastOrder**: Reflect customer momentum and recent engagement, which correlate with satisfaction levels.

#### Business Implication:
These insights help identify key drivers of satisfaction and guide operational and strategic improvements. For instance:
- Optimizing delivery logistics (`WarehouseToHome`)
- Personalizing retention strategies based on `Tenure` and `AvgOrderValue`
- Enhancing post-purchase experiences for customers showing lower satisfaction scores

This feature importance visualization also aids transparency, making the model more interpretable and actionable for stakeholders.

- **WarehouseToHome** measures the distance between fulfillment and the customer. Customers closer to the warehouse—who likely receive faster delivery—tend to report higher satisfaction.

#### Why Warehouse Proximity Matters:
While some may argue that building or optimizing warehouse infrastructure could be costly, **logistics efficiency has been repeatedly shown to boost customer satisfaction and loyalty**.

- According to **DHL** (2023), *“the final mile can be the make-or-break moment in the customer journey,”* emphasizing the importance of delivery speed and reliability ([DHL article](https://www.dhl.com/discover/en-us/global-logistics-advice/import-export-advice/last-mile-solutions)).
- **ShipBob**, a leading e-commerce fulfillment provider, explains that faster fulfillment via strategically located warehouses **reduces cart abandonment and increases retention**, both of which enhance LTV ([ShipBob article](https://www.shipbob.com/blog/how-last-mile-delivery-works/)).
- **Aero Fulfillment Systems** highlights that **modern warehousing has shifted toward being a customer experience differentiator**, not just a backend cost center ([Aero article](https://www.aerofulfillment.com/evolution-of-modern-warehousing/)).

#### Balancing Cost with Value:
While warehouse expansion or repositioning does require investment, our model shows that `WarehouseToHome` is strongly associated with satisfaction. This supports the idea that improving fulfillment speed could **reduce churn and increase repeat purchases**, ultimately offsetting upfront logistical costs.

#### Recommendation:
Rather than suggesting a full infrastructure overhaul, we recommend:
- Exploring **micro-fulfillment centers** in high-density zones.
- Offering premium shipping options for high-value customers.
- Continuously A/B testing delivery satisfaction vs. fulfillment cost.

By integrating model insights with industry research, this strategy remains both data-backed and financially considerate.
"""

# Cross-validation F1
f1_cv = cross_val_score(grid_search.best_estimator_, X_train, y_train, cv=5, scoring='f1_macro')
print("Mean CV F1 Score:", f1_cv.mean())

# XGBoost & LightGBM
y_encoded = LabelEncoder().fit_transform(y)
xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
xgb_grid = GridSearchCV(xgb, {'n_estimators': [100, 200], 'max_depth': [3, 5], 'learning_rate': [0.01, 0.1]}, scoring='f1_macro', cv=5)
xgb_grid.fit(X, y_encoded)
print("Best XGB F1 Macro:", xgb_grid.best_score_)

lgbm = LGBMClassifier(random_state=42)
lgbm_grid = GridSearchCV(lgbm, {'n_estimators': [100, 200], 'max_depth': [5, -1], 'learning_rate': [0.01, 0.1]}, scoring='f1_macro', cv=5)
lgbm_grid.fit(X, y_encoded)
print("Best LGBM F1 Macro:", lgbm_grid.best_score_)

"""### Customer Satisfaction Prediction: Model Comparison Summary

We tested several machine learning models to classify customer satisfaction into three levels: Low, Medium, and High. The models were evaluated using cross-validation F1 Macro scores to account for class imbalance and multiclass prediction.

#### Model Performance Summary:

| Model                  | Evaluation Metric     | Score     |
|------------------------|------------------------|-----------|
| **Random Forest**      | Mean CV F1 Macro       | **0.813** |
| **XGBoost**            | Best CV F1 Macro       | 0.610     |
| **LightGBM**           | Best CV F1 Macro       | **0.816** |

#### Interpretation:
- **LightGBM** achieved the highest F1 Macro score (0.816), slightly outperforming Random Forest (0.813). This makes LightGBM the most effective model for capturing satisfaction levels across all classes.
- **XGBoost** underperformed in comparison, with a substantially lower F1 Macro (0.610), suggesting it may not be well-suited for this specific classification task without additional tuning or feature engineering.
- Random Forest remains a strong alternative due to its interpretability, competitive accuracy, and strong class-wise balance.

#### Conclusion & Recommendation:
Both Random Forest and LightGBM are viable candidates for satisfaction classification. However, given its superior macro performance, **LightGBM is recommended as the production model**—especially when accurate performance across all satisfaction levels is critical.

For stakeholder transparency, the Random Forest model’s feature importance insights can still be leveraged to explain key drivers of satisfaction.

## Final Reflections & Summary

### 1. Problem Statement Recap
This project addresses two critical challenges in e-commerce:
- **Predicting customer churn**, which is essential for proactive retention efforts.
- **Predicting customer satisfaction**, which informs long-term value, loyalty, and experience improvement.

By solving these problems, the business can better target retention strategies and improve operational decisions based on predictive insights.

---

### 2. Data Description
The dataset includes **5630 records** with **20+ features** covering:
- Customer demographics (Gender, Marital Status, City Tier)
- Behavioral metrics (Tenure, OrderCount, CashbackAmount, HourSpendOnApp)
- Engagement signals (Complaint raised, Days Since Last Order, Coupons used)
- Satisfaction Score (1–5) and Churn status (binary)

We used these features to engineer new predictors (e.g., `AvgOrderValue`, `CouponRedemptionRate`) and build models for both binary and multiclass classification.

---

### 3. Thought Process & Analysis Flow
Our analysis followed this structured workflow:
1. **Data cleaning & feature engineering**
2. **Exploratory data analysis** (univariate, bivariate, multivariate)
3. **Hypothesis testing** to explore causal claims (e.g., coupons and order value)
4. **Churn prediction** using Logistic Regression, Random Forest, and SVM
5. **Satisfaction prediction** using Random Forest, XGBoost, and LightGBM
6. **Threshold tuning & model evaluation**
7. **Feature importance interpretation** and business recommendations

Each step informed the next, ensuring our models were data-informed and grounded in exploratory findings.

---

### 4. Justification for Models & Techniques
- **Logistic Regression**: Baseline model for binary churn classification; interpretable.
- **Random Forest**: Handles non-linearity and interactions; used in both tasks.
- **SVM**: Strong performer for churn with limited data and clear boundaries.
- **XGBoost/LightGBM**: Gradient boosting models designed for multiclass classification with better regularization and speed.

We used **GridSearchCV** with cross-validation for fair comparison and **threshold tuning** to optimize for F1-score, which balances precision and recall for imbalanced churn labels.

---

### 5. Preprocessing Highlights
- Dropped ~5% rows with missing values (low, evenly distributed).
- Standardized continuous features (for models like SVM/LogReg).
- One-hot encoded categorical variables (e.g., PaymentMode, Device).
- Label encoded ordinal targets (e.g., SatisfactionCategory).
- Created new features like:
  - `AvgOrderValue = CashbackAmount / OrderCount`
  - `CouponRedemptionRate = CouponUsed / OrderCount`

---

### 6. Model Usefulness & Applications
The trained models have clear business applications:
- **Churn model** flags customers at risk of leaving, enabling targeted retention offers, proactive support, and personalized outreach.
- **Satisfaction model** helps identify dissatisfied users early, optimize service experience, and segment customers by engagement quality.

These insights can drive:
- Loyalty program design
- Resource allocation (e.g., warehouse strategy, customer service staffing)
- CX-driven product improvements

---

### 7. Limitations
- **Satisfaction labels may contain subjectivity** or inconsistency.
- **Some variables (e.g., complaints)** may be lagging indicators—not helpful in early intervention.
- **Random forest interpretability** can be limited despite strong performance.
- **Class imbalance**, especially in satisfaction categories, could affect model learning despite F1 tuning.
- The `WarehouseToHome` strategy, while predictive, may raise concerns about cost—which we addressed using external logistics literature.

---

### Citations & Resources
- [DHL on Last Mile](https://www.dhl.com/discover/en-us/global-logistics-advice/import-export-advice/last-mile-solutions)
- [ShipBob on Last-Mile Efficiency](https://www.shipbob.com/blog/how-last-mile-delivery-works/)
- [Aero Fulfillment on Warehouse Evolution](https://www.aerofulfillment.com/evolution-of-modern-warehousing/)
- scikit-learn documentation
- Stack Overflow for code syntax adjustments and GridSearch tips
"""

